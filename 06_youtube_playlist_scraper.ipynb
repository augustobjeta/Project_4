{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f516d98-97e8-4d7b-99be-61e9100f1998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mastermind. The base of the code in this noteobook is from the below repo.\n",
    "# https://github.com/analyticswithadam/Python/blob/main/Pull_all_Comments_and_Replies_for_YouTube_Playlists.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b58f5dd-5008-41a4-920f-01e95de5307a",
   "metadata": {},
   "source": [
    "# Data scraping youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27f6d584-092c-468d-96ec-66c10a186c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import getpass\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "da8b427a-1295-41a7-b5f0-f088743c32ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your YouTube API key:  ········\n"
     ]
    }
   ],
   "source": [
    "api_key = getpass.getpass('Please enter your YouTube API key: ')\n",
    "# You will need ot obtain one of these googlequery\n",
    "playlist_ids = ['PL3-OIwNPoC3KQ4d8hMwGIQnBB4A3Dm3UO']\n",
    "# These can be found in the read me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "269fda0a-e234-4359-836d-ecefd7a987d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the YouTube client\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "68d9f493-4433-49a0-be5b-4654c4f692fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function to pull all comments from a youtube playlist. \n",
    "# Only include the text following the signin the playlists url.\n",
    "\n",
    "\n",
    "def get_all_video_ids_from_playlists(youtube, playlist_ids):\n",
    "    all_videos = []  # Initialize a single list to hold all video IDs\n",
    "\n",
    "    for playlist_id in playlist_ids:\n",
    "        next_page_token = None\n",
    "\n",
    "        # Fetch videos from the current playlist\n",
    "        while True:\n",
    "            playlist_request = youtube.playlistItems().list(\n",
    "                part='contentDetails',\n",
    "                playlistId=playlist_id,\n",
    "                maxResults=50,\n",
    "                pageToken=next_page_token)\n",
    "            playlist_response = playlist_request.execute()\n",
    "\n",
    "            all_videos += [item['contentDetails']['videoId'] for item in playlist_response['items']]\n",
    "\n",
    "            next_page_token = playlist_response.get('nextPageToken')\n",
    "\n",
    "            if next_page_token is None:\n",
    "                break\n",
    "\n",
    "    return all_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4e3d591a-7781-42ab-b417-08ae3ff02478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all video IDs from the specified playlists\n",
    "video_ids = get_all_video_ids_from_playlists(youtube, playlist_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "25f0152b-242e-4e29-8fc1-7428b5fd8fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dates to convert to obtain comments within date range.\n",
    "# set to desired range\n",
    "START_DATE_STR = \"2024-01-01\"  \n",
    "END_DATE_STR = \"2024-11-04\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "43887b76-fd63-4633-b539-4705a988dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.google.com/search?q=only+scrape+comments+between+a+certain+date+youtube+code&sca_esv=66874c16b3ccadbb&ei=p72KaK-NLPCJwbkPw5C5kA0&ved=0ahUKEwjv_KGj8OWOAxXwRDABHUNIDtIQ4dUDCBA&uact=5&oq=only+scrape+comments+between+a+certain+date+youtube+code&gs_lp=Egxnd3Mtd2l6LXNlcnAiOG9ubHkgc2NyYXBlIGNvbW1lbnRzIGJldHdlZW4gYSBjZXJ0YWluIGRhdGUgeW91dHViZSBjb2RlMgUQIRigATIFECEYoAEyBRAhGKABMgUQIRigATIFECEYoAFIuAtQnQRYnApwAHgCkAEAmAFloAGkA6oBAzQuMbgBA8gBAPgBAZgCBqACtgPCAgQQABhHwgIFECEYnwXCAgUQIRirApgDAOIDBRIBMSBAiAYBkAYIkgcDNS4xoAfDJLIHAzQuMbgHswPCBwMwLjbIBwo&sclient=gws-wiz-serp\n",
    "# Convert date strings to datetime objects. Added to exclude comments post election.\n",
    "start_date = datetime.datetime.strptime(START_DATE_STR, \"%Y-%m-%d\").date()\n",
    "end_date = datetime.datetime.strptime(END_DATE_STR, \"%Y-%m-%d\").date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e736520e-ff3e-45f2-a91a-a82379c76f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET ALL COMMENTS!!!\n",
    "\n",
    "# Fetch all video IDs from the specified playlists\n",
    "video_ids = get_all_video_ids_from_playlists(youtube, playlist_ids)\n",
    "\n",
    "# Function to get replies for a specific comment\n",
    "def get_replies(youtube, parent_id, video_id):  # Added video_id as an argument\n",
    "    replies = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        reply_request = youtube.comments().list(\n",
    "            part=\"snippet\",\n",
    "            parentId=parent_id,\n",
    "            textFormat=\"plainText\",\n",
    "            maxResults=100,\n",
    "            order=\"time\",\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        reply_response = reply_request.execute()\n",
    "\n",
    "        for item in reply_response['items']:\n",
    "            comment = item['snippet']\n",
    "            replies.append({\n",
    "                'Timestamp': comment['publishedAt'],\n",
    "                'Username': comment['authorDisplayName'],\n",
    "                'VideoID': video_id,\n",
    "                'Comment': comment['textDisplay'],\n",
    "                'Date': comment['updatedAt'] if 'updatedAt' in comment else comment['publishedAt']\n",
    "            })\n",
    "\n",
    "        next_page_token = reply_response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return replies\n",
    "\n",
    "# Function to get all comments (including replies) for a single video\n",
    "def get_comments_for_video(youtube, video_id):\n",
    "    all_comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            comment_request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\",\n",
    "                order=\"time\",\n",
    "                maxResults=100\n",
    "            )\n",
    "            comment_response = comment_request.execute()\n",
    "    \n",
    "            for item in comment_response['items']:\n",
    "                top_comment = item['snippet']['topLevelComment']['snippet']\n",
    "                comment_date_str = top_comment[\"publishedAt\"].split(\"T\")[0]  # Extract date part only\n",
    "                comment_date = datetime.datetime.strptime(comment_date_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "                # Filter comments based on the specified date range\n",
    "                if start_date <= comment_date <= end_date:\n",
    "                    all_comments.append({\n",
    "                        'Timestamp': top_comment['publishedAt'],\n",
    "                        'Username': top_comment['authorDisplayName'],\n",
    "                        'VideoID': video_id,  # Directly using video_id from function parameter\n",
    "                        'Comment': top_comment['textDisplay'],\n",
    "                        'Date': top_comment['updatedAt'] if 'updatedAt' in top_comment else top_comment['publishedAt']\n",
    "                    })\n",
    "                elif comment_date < start_date:\n",
    "                    # If we've reached comments older than the start date, stop fetching\n",
    "                    break  # Exit the loop since comments are ordered by time\n",
    "\n",
    "    \n",
    "                # # Fetch replies if there are any\n",
    "                # if item['snippet']['totalReplyCount'] > 0:\n",
    "                #     all_comments.extend(get_replies(youtube, item['snippet']['topLevelComment']['id'], video_id))\n",
    "    \n",
    "            next_page_token = comment_response.get('nextPageToken')\n",
    "            if not next_page_token or comment_date < start_date:  # Stop if no more pages or outside date range\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "    return all_comments\n",
    "\n",
    "# List to hold all comments from all videos\n",
    "all_comments = []\n",
    "\n",
    "\n",
    "for video_id in video_ids:\n",
    "    video_comments = get_comments_for_video(youtube, video_id)\n",
    "    all_comments.extend(video_comments)\n",
    "\n",
    "# Create DataFrame\n",
    "comments_df = pd.DataFrame(all_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7d8fb80b-348f-4b20-8a80-7b1ba85aafb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.to_csv('./scrapped_date/2024Rep_debate_comments.csv', index=False)\n",
    "# Use approriate csv name and location\n",
    "# TO APPEND TO CSV:\n",
    "# new_df.to_csv('existing_data.csv', mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9a1ecbe4-7774-4c15-a00d-0f706f25e640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51652, 5)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how many observations were captured\n",
    "df = pd.read_csv('2024Rep_debate_comments.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed785f0e-cbe2-4319-8f44-1f1749834dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the first scraping 2016 we didnt remove comment replies \n",
    "# so added a second cell to check the change in number of comments without replies\n",
    "df = pd.read_csv('2024Rep_debate_comments2.csv')\n",
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
