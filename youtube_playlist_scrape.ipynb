{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f516d98-97e8-4d7b-99be-61e9100f1998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mastermind\n",
    "# https://github.com/analyticswithadam/Python/blob/main/Pull_all_Comments_and_Replies_for_YouTube_Playlists.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27f6d584-092c-468d-96ec-66c10a186c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import getpass\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "da8b427a-1295-41a7-b5f0-f088743c32ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your YouTube API key:  ········\n"
     ]
    }
   ],
   "source": [
    "api_key = getpass.getpass('Please enter your YouTube API key: ')\n",
    "playlist_ids = ['PL3-OIwNPoC3KQ4d8hMwGIQnBB4A3Dm3UO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "269fda0a-e234-4359-836d-ecefd7a987d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the YouTube client\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "68d9f493-4433-49a0-be5b-4654c4f692fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function to pull all comments from a youtube playlist. \n",
    "# Only include the text following the signin the playlists url.\n",
    "\n",
    "\n",
    "def get_all_video_ids_from_playlists(youtube, playlist_ids):\n",
    "    all_videos = []  # Initialize a single list to hold all video IDs\n",
    "\n",
    "    for playlist_id in playlist_ids:\n",
    "        next_page_token = None\n",
    "\n",
    "        # Fetch videos from the current playlist\n",
    "        while True:\n",
    "            playlist_request = youtube.playlistItems().list(\n",
    "                part='contentDetails',\n",
    "                playlistId=playlist_id,\n",
    "                maxResults=50,\n",
    "                pageToken=next_page_token)\n",
    "            playlist_response = playlist_request.execute()\n",
    "\n",
    "            all_videos += [item['contentDetails']['videoId'] for item in playlist_response['items']]\n",
    "\n",
    "            next_page_token = playlist_response.get('nextPageToken')\n",
    "\n",
    "            if next_page_token is None:\n",
    "                break\n",
    "\n",
    "    return all_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4e3d591a-7781-42ab-b417-08ae3ff02478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all video IDs from the specified playlists\n",
    "video_ids = get_all_video_ids_from_playlists(youtube, playlist_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "25f0152b-242e-4e29-8fc1-7428b5fd8fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dates to convert to obtain comments within date range.\n",
    "# set to desired range\n",
    "START_DATE_STR = \"2024-01-01\"  \n",
    "END_DATE_STR = \"2024-11-04\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "43887b76-fd63-4633-b539-4705a988dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.google.com/search?q=only+scrape+comments+between+a+certain+date+youtube+code&sca_esv=66874c16b3ccadbb&ei=p72KaK-NLPCJwbkPw5C5kA0&ved=0ahUKEwjv_KGj8OWOAxXwRDABHUNIDtIQ4dUDCBA&uact=5&oq=only+scrape+comments+between+a+certain+date+youtube+code&gs_lp=Egxnd3Mtd2l6LXNlcnAiOG9ubHkgc2NyYXBlIGNvbW1lbnRzIGJldHdlZW4gYSBjZXJ0YWluIGRhdGUgeW91dHViZSBjb2RlMgUQIRigATIFECEYoAEyBRAhGKABMgUQIRigATIFECEYoAFIuAtQnQRYnApwAHgCkAEAmAFloAGkA6oBAzQuMbgBA8gBAPgBAZgCBqACtgPCAgQQABhHwgIFECEYnwXCAgUQIRirApgDAOIDBRIBMSBAiAYBkAYIkgcDNS4xoAfDJLIHAzQuMbgHswPCBwMwLjbIBwo&sclient=gws-wiz-serp\n",
    "# Convert date strings to datetime objects. Added to exclude comments post election.\n",
    "start_date = datetime.datetime.strptime(START_DATE_STR, \"%Y-%m-%d\").date()\n",
    "end_date = datetime.datetime.strptime(END_DATE_STR, \"%Y-%m-%d\").date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e736520e-ff3e-45f2-a91a-a82379c76f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET ALL COMMENTS!!!\n",
    "\n",
    "# Fetch all video IDs from the specified playlists\n",
    "video_ids = get_all_video_ids_from_playlists(youtube, playlist_ids)\n",
    "\n",
    "# Function to get replies for a specific comment\n",
    "def get_replies(youtube, parent_id, video_id):  # Added video_id as an argument\n",
    "    replies = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        reply_request = youtube.comments().list(\n",
    "            part=\"snippet\",\n",
    "            parentId=parent_id,\n",
    "            textFormat=\"plainText\",\n",
    "            maxResults=100,\n",
    "            order=\"time\",\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        reply_response = reply_request.execute()\n",
    "\n",
    "        for item in reply_response['items']:\n",
    "            comment = item['snippet']\n",
    "            replies.append({\n",
    "                'Timestamp': comment['publishedAt'],\n",
    "                'Username': comment['authorDisplayName'],\n",
    "                'VideoID': video_id,\n",
    "                'Comment': comment['textDisplay'],\n",
    "                'Date': comment['updatedAt'] if 'updatedAt' in comment else comment['publishedAt']\n",
    "            })\n",
    "\n",
    "        next_page_token = reply_response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return replies\n",
    "\n",
    "# Function to get all comments (including replies) for a single video\n",
    "def get_comments_for_video(youtube, video_id):\n",
    "    all_comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            comment_request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\",\n",
    "                order=\"time\",\n",
    "                maxResults=100\n",
    "            )\n",
    "            comment_response = comment_request.execute()\n",
    "    \n",
    "            for item in comment_response['items']:\n",
    "                top_comment = item['snippet']['topLevelComment']['snippet']\n",
    "                comment_date_str = top_comment[\"publishedAt\"].split(\"T\")[0]  # Extract date part only\n",
    "                comment_date = datetime.datetime.strptime(comment_date_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "                # Filter comments based on the specified date range\n",
    "                if start_date <= comment_date <= end_date:\n",
    "                    all_comments.append({\n",
    "                        'Timestamp': top_comment['publishedAt'],\n",
    "                        'Username': top_comment['authorDisplayName'],\n",
    "                        'VideoID': video_id,  # Directly using video_id from function parameter\n",
    "                        'Comment': top_comment['textDisplay'],\n",
    "                        'Date': top_comment['updatedAt'] if 'updatedAt' in top_comment else top_comment['publishedAt']\n",
    "                    })\n",
    "                elif comment_date < start_date:\n",
    "                    # If we've reached comments older than the start date, stop fetching\n",
    "                    break  # Exit the loop since comments are ordered by time\n",
    "\n",
    "    \n",
    "                # # Fetch replies if there are any\n",
    "                # if item['snippet']['totalReplyCount'] > 0:\n",
    "                #     all_comments.extend(get_replies(youtube, item['snippet']['topLevelComment']['id'], video_id))\n",
    "    \n",
    "            next_page_token = comment_response.get('nextPageToken')\n",
    "            if not next_page_token or comment_date < start_date:  # Stop if no more pages or outside date range\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "    return all_comments\n",
    "\n",
    "# List to hold all comments from all videos\n",
    "all_comments = []\n",
    "\n",
    "\n",
    "for video_id in video_ids:\n",
    "    video_comments = get_comments_for_video(youtube, video_id)\n",
    "    all_comments.extend(video_comments)\n",
    "\n",
    "# Create DataFrame\n",
    "comments_df = pd.DataFrame(all_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7d8fb80b-348f-4b20-8a80-7b1ba85aafb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.to_csv('2024Rep_debate_comments.csv', index=False)\n",
    "# # TO APPEND TO CSV:\n",
    "# # new_df.to_csv('existing_data.csv', mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9a1ecbe4-7774-4c15-a00d-0f706f25e640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51652, 5)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how many observations were captured\n",
    "df = pd.read_csv('2024Rep_debate_comments.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed785f0e-cbe2-4319-8f44-1f1749834dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the first scraping 2016 we didt remove comment replies \n",
    "# so added a second cell to check the change in number of comments without replies\n",
    "df = pd.read_csv('trump_comments2.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa17e4ab-5bda-4f1e-8eb5-7514656d4936",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1dde11-2d4a-42b7-86e6-5cc4d36fdf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKED GREAT FOR COMMENTS FROM ONE VIDEO\n",
    "\n",
    "\n",
    "def getcomments(video):\n",
    "  request = youtube.commentThreads().list(\n",
    "      part=\"snippet\",\n",
    "      videoId=video,\n",
    "      maxResults=100\n",
    "  )\n",
    "\n",
    "  comments = []\n",
    "\n",
    "  # Execute the request.\n",
    "  response = request.execute()\n",
    "\n",
    "  # Get the comments from the response.\n",
    "  for item in response['items']:\n",
    "      comment = item['snippet']['topLevelComment']['snippet']\n",
    "      public = item['snippet']['isPublic']\n",
    "      comments.append([\n",
    "          comment['authorDisplayName'],\n",
    "          comment['publishedAt'],\n",
    "          comment['likeCount'],\n",
    "          comment['textOriginal'],\n",
    "          comment['videoId'],\n",
    "          public\n",
    "      ])\n",
    "\n",
    "  while (1 == 1):\n",
    "    try:\n",
    "     nextPageToken = response['nextPageToken']\n",
    "    except KeyError:\n",
    "     break\n",
    "    nextPageToken = response['nextPageToken']\n",
    "    # Create a new request object with the next page token.\n",
    "    nextRequest = youtube.commentThreads().list(part=\"snippet\", videoId=video, maxResults=100, pageToken=nextPageToken)\n",
    "    # Execute the next request.\n",
    "    response = nextRequest.execute()\n",
    "    # Get the comments from the next response.\n",
    "    for item in response['items']:\n",
    "      comment = item['snippet']['topLevelComment']['snippet']\n",
    "      public = item['snippet']['isPublic']\n",
    "      comments.append([\n",
    "          comment['authorDisplayName'],\n",
    "          comment['publishedAt'],\n",
    "          comment['likeCount'],\n",
    "          comment['textOriginal'],\n",
    "          comment['videoId'],\n",
    "          public\n",
    "      ])\n",
    "\n",
    "  df2 = pd.DataFrame(comments, columns=['author', 'updated_at', 'like_count', 'text','video_id','public'])\n",
    "  return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165fc44-5f81-4d29-ae65-ce189280742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# request = youtube.commentThreads().list(\n",
    "#     part=\"snippet\",\n",
    "#     videoId=\"ltNVyvK8Paw\",\n",
    "#     maxResults=1000\n",
    "# )\n",
    "# response = request.execute()\n",
    "\n",
    "# comments = []\n",
    "\n",
    "# for item in response['items']:\n",
    "#     comment = item['snippet']['topLevelComment']['snippet']\n",
    "#     comments.append([\n",
    "#         comment['authorDisplayName'],\n",
    "#         comment['publishedAt'],\n",
    "#         comment['updatedAt'],\n",
    "#         comment['likeCount'],\n",
    "#         comment['textDisplay']\n",
    "#     ])\n",
    "\n",
    "\n",
    "# # TO APPEND TO CSV:\n",
    "# # new_df.to_csv('existing_data.csv', mode='a', index=False, header=False)\n",
    "# df = pd.DataFrame(comments, columns=['author', 'published_at', 'updated_at', 'like_count', 'text'])\n",
    "\n",
    "# df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ad558b-0ad3-4227-bb3c-79d9b02e961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.to_csv('existing_data.csv', mode='a', index=False, header=False)\n",
    "\n",
    "\n",
    "df.to_csv('trump_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb2fcb-6be3-4162-9d9f-eb0d20dd6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export whole dataset to the local machine as CSV File\n",
    "# csv_file = 'comments_data.csv'  # Name your file\n",
    "# comments_df.to_csv(csv_file, index=False)\n",
    "\n",
    "# from google.colab import files\n",
    "\n",
    "# # Trigger a download to your local machine\n",
    "# files.download(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f534deca-b5a2-4444-bc47-d8e46f87effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to get replies for a specific comment\n",
    "# def get_replies(youtube, parent_id, video_id):  # Added video_id as an argument\n",
    "#     replies = []\n",
    "#     next_page_token = None\n",
    "\n",
    "#     while True:\n",
    "#         reply_request = youtube.comments().list(\n",
    "#             part=\"snippet\",\n",
    "#             parentId=parent_id,\n",
    "#             textFormat=\"plainText\",\n",
    "#             maxResults=100,\n",
    "#             pageToken=next_page_token\n",
    "#         )\n",
    "#         reply_response = reply_request.execute()\n",
    "\n",
    "#         for item in reply_response['items']:\n",
    "#             comment = item['snippet']\n",
    "#             replies.append({\n",
    "#                 'Timestamp': comment['publishedAt'],\n",
    "#                 'Username': comment['authorDisplayName'],\n",
    "#                 'VideoID': video_id,\n",
    "#                 'Comment': comment['textDisplay'],\n",
    "#                 'Date': comment['updatedAt'] if 'updatedAt' in comment else comment['publishedAt']\n",
    "#             })\n",
    "\n",
    "#         next_page_token = reply_response.get('nextPageToken')\n",
    "#         if not next_page_token:\n",
    "#             break\n",
    "\n",
    "#     return replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e5d5f7-d877-44f8-84e2-1ef71be201a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to get all comments (including replies) for a single video\n",
    "# def get_comments_for_video(youtube, video_id):\n",
    "#     all_comments = []\n",
    "#     next_page_token = None\n",
    "\n",
    "#     while True:\n",
    "#         comment_request = youtube.commentThreads().list(\n",
    "#             part=\"snippet\",\n",
    "#             videoId=video_id,\n",
    "#             pageToken=next_page_token,\n",
    "#             textFormat=\"plainText\",\n",
    "#             maxResults=100\n",
    "#         )\n",
    "#         comment_response = comment_request.execute()\n",
    "\n",
    "#         for item in comment_response['items']:\n",
    "#             top_comment = item['snippet']['topLevelComment']['snippet']\n",
    "#             all_comments.append({\n",
    "#                 'Timestamp': top_comment['publishedAt'],\n",
    "#                 'Username': top_comment['authorDisplayName'],\n",
    "#                 'VideoID': video_id,  # Directly using video_id from function parameter\n",
    "#                 'Comment': top_comment['textDisplay'],\n",
    "#                 'Date': top_comment['updatedAt'] if 'updatedAt' in top_comment else top_comment['publishedAt']\n",
    "#             })\n",
    "\n",
    "#             # Fetch replies if there are any\n",
    "#             if item['snippet']['totalReplyCount'] > 0:\n",
    "#                 all_comments.extend(get_replies(youtube, item['snippet']['topLevelComment']['id'], video_id))\n",
    "\n",
    "#         next_page_token = comment_response.get('nextPageToken')\n",
    "#         if not next_page_token:\n",
    "#             break\n",
    "\n",
    "#     return all_comments\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
